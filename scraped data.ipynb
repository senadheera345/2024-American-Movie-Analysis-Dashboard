{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a8fe35e-62e2-4402-9920-335463ce59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6cbf77b7-48cd-4e20-baed-7482fb0ec283",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://en.wikipedia.org/wiki/List_of_American_films_of_2024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ed30b65f-e713-4dfc-9410-e89a5fd62fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = ({'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36' ,'Accept-Language': 'en-US, en;q=0.5' })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fa4a6f9c-7199-4eae-91c9-bd19660c7ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_page = requests.get(URL , headers=HEADER)\n",
    "web_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "34d25472-559f-404c-a2a3-b9e14f1d4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert into HTML format\n",
    "soup = BeautifulSoup(web_page.content , \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "d11b273d-0e8d-498d-9308-6ee034a42bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank                            Title             Distributor Domestic gross\n",
      "0    1                     Inside Out 2                  Disney   $652,980,194\n",
      "1    2             Deadpool & Wolverine            $636,742,741           None\n",
      "2    3                          Wicked†               Universal   $460,576,550\n",
      "3    4                         Moana 2†                  Disney   $436,353,939\n",
      "4    5                  Despicable Me 4               Universal   $361,004,205\n",
      "5    6          Beetlejuice Beetlejuice            Warner Bros.   $294,097,060\n",
      "6    7                   Dune: Part Two            $282,144,358           None\n",
      "7    8                         Twisters  Universal/Warner Bros.   $267,762,265\n",
      "8    9            Sonic the Hedgehog 3†               Paramount   $207,311,800\n",
      "9   10  Godzilla x Kong: The New Empire            Warner Bros.   $196,350,016\n",
      "Data written to highest_grossing_films_2024.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract the table with the specified class\n",
    "table = soup.find(\"table\", class_=\"wikitable sortable\")\n",
    "if table:\n",
    "    # Extract all rows of data including the header row\n",
    "    rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "    # Extract headers from the first row\n",
    "    headers = [header.get_text(strip=True) for header in rows[0].find_all(\"th\")]\n",
    "\n",
    "    # Extract data from the remaining rows\n",
    "    data = []\n",
    "    for row in rows[1:]:  # Skip the header row\n",
    "        values = [cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]\n",
    "        data.append(values)\n",
    "\n",
    "    # Convert to a DataFrame for better visualization\n",
    "    df = pd.DataFrame(data, columns=headers)\n",
    "    print(df)\n",
    "\n",
    "    # Write the DataFrame to a CSV file\n",
    "    df.to_csv(\"highest_grossing_films_2024.csv\", index=False)\n",
    "    print(\"Data written to highest_grossing_films_2024.csv\")\n",
    "else:\n",
    "    print(\"Table not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01736cfd-c01f-4a11-a6ed-0e3ef4d3c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "table = soup.find(\"table\", {\"class\": lambda x: x and \"wikitable\" in x})\n",
    "table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "10a2cee3-8cc4-4225-b09f-989212518763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to highest_grossing_films_2024_table_1.csv\n",
      "Data written to highest_grossing_films_2024_table_2.csv\n",
      "Data written to highest_grossing_films_2024_table_3.csv\n",
      "Data written to highest_grossing_films_2024_table_4.csv\n",
      "Data written to highest_grossing_films_2024_table_5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assuming 'soup' is your parsed HTML content\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable sortable\")\n",
    "\n",
    "if tables:\n",
    "    # Iterate over all found tables\n",
    "    for index, table in enumerate(tables, start=1):  # Enumerate for generating a unique filename\n",
    "        # Extract all rows of data including the header row\n",
    "        rows = table.find(\"tbody\").find_all(\"tr\")\n",
    "\n",
    "        # Extract headers from the first row, or use default headers if not found\n",
    "        headers = [header.get_text(strip=True) for header in rows[0].find_all(\"th\")]\n",
    "        \n",
    "        # If no headers are found, assign default headers\n",
    "        if not headers:\n",
    "            headers = [f\"Column {i+1}\" for i in range(len(rows[1].find_all(\"td\")))]\n",
    "        \n",
    "        # Extract data from the remaining rows\n",
    "        data = []\n",
    "        for row in rows[1:]:  # Skip the header row\n",
    "            cells = row.find_all(['th', 'td'])\n",
    "            values = [cell.get_text(strip=True) for cell in cells]\n",
    "\n",
    "            # Ensure data matches the number of columns\n",
    "            if len(values) < len(headers):\n",
    "                values.extend([\"\"] * (len(headers) - len(values)))  # Pad missing columns with empty values\n",
    "            elif len(values) > len(headers):\n",
    "                values = values[:len(headers)]  # Truncate extra columns\n",
    "\n",
    "            data.append(values)\n",
    "\n",
    "        # Convert to a DataFrame for better visualization\n",
    "        df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "        # Write the DataFrame to a CSV file for each table\n",
    "        csv_filename = f\"highest_grossing_films_2024_table_{index}.csv\"\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        print(f\"Data written to {csv_filename}\")\n",
    "else:\n",
    "    print(\"No table found with class 'wikitable sortable'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
